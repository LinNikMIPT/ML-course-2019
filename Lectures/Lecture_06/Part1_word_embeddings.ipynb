{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Некоторые модели очень тяжелые, поэтому советую пользоваться Google Colab. Ссылка на ноутбук в коллабе:\n",
    "\n",
    "https://colab.research.google.com/drive/1mD6G0RN2tPAPE5CQcDaRCXiPA63ag8OY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <center> Word embeddings <center>\n",
    "\n",
    "**Word embedding** -  technique in NLP where words or phrases from the vocabulary are mapped to vectors of real numbers.\n",
    "\n",
    "Called an **\"embedding\"** because it's embedded into a space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two kinds of embeddings\n",
    "**Sparse** (e.g. TF-IDF, One-Hot-Enc, PPMI)\n",
    "- a common baseline model\n",
    "- Sparse vectors\n",
    "- Words are represented by a simple function of the counts of nearby words\n",
    "\n",
    "**Dense** (e.g. word2vec, doc2vec)\n",
    "- Dense vectors\n",
    "- Representation is created by training a classifier to distinguish nearby and far-away words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sky</th>\n",
       "      <th>is</th>\n",
       "      <th>blue</th>\n",
       "      <th>She</th>\n",
       "      <th>getting</th>\n",
       "      <th>better</th>\n",
       "      <th>Everything</th>\n",
       "      <th>possible</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Sky</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>is</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>blue</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>She</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>getting</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>better</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Everything</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>possible</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Sky  is  blue  She  getting  better  Everything  possible\n",
       "Sky         1.0   1   1.0  0.0      0.0     0.0         0.0       0.0\n",
       "is          1.0   1   1.0  1.0      1.0     1.0         1.0       1.0\n",
       "blue        1.0   1   1.0  0.0      0.0     0.0         0.0       0.0\n",
       "She         0.0   1   0.0  1.0      1.0     1.0         0.0       0.0\n",
       "getting     0.0   1   0.0  1.0      1.0     1.0         0.0       0.0\n",
       "better      0.0   1   0.0  1.0      1.0     1.0         0.0       0.0\n",
       "Everything  0.0   1   0.0  0.0      0.0     0.0         1.0       1.0\n",
       "possible    0.0   1   0.0  0.0      0.0     0.0         1.0       1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example of vector represenation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import defaultdict\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Lets for example consider a simple way to map words from sentences into dense vectors.\n",
    "# Lets make a table with words coocurrencies and then project vectors of all words into 2D using PCA.\n",
    "\n",
    "s = ['Sky is blue', 'She is getting better', 'Everything is possible']\n",
    "dic = defaultdict(dict)\n",
    "for sent in s:\n",
    "    words = sent.split()\n",
    "    for w in words:\n",
    "        for w2 in words:\n",
    "            dic[w][w2]=1\n",
    "\n",
    "df = pd.DataFrame(dic)\n",
    "df.fillna(0, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = PCA().fit_transform(df)\n",
    "\n",
    "font = {'size'   : 15}\n",
    "plt.rc('font', **font)\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(res[:,0], res[:,1])\n",
    "plt.axis('off')\n",
    "for i, label in enumerate(df.columns):\n",
    "    x, y = res[i,0], res[i,1]\n",
    "    plt.scatter(x, y)\n",
    "    annot = {'has': (1, 50), 'is': (1, 5)}\n",
    "    plt.annotate(label, xy=(x, y),\n",
    "                 xytext=annot.get(label,(1+i*2, 6*i)), \n",
    "                 textcoords='offset points',\n",
    "                   ha='right', va='bottom', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **Each word = a vector**\n",
    " - **Similar words are “nearby in space”**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Что такое Word2Vec?\n",
    "\n",
    "**Idea:** predict rather than count (*Mikolov et al., 2013)\n",
    "\n",
    "Instead of counting how often each word w occurs near \"apricot” train a classifier on a binary prediction task:\n",
    "\n",
    "–Is w likely to show up near \"apricot\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec – одна из технологий анализа семантики естественных языков, которая основана на векторном представлении слов согласно их семантической близости. Был разработан группой исследователей Google в 2013 году, активно применяется для семантического анализа текстов и похожих задачах.\n",
    "Word2vec в качестве входных данных принимает текстовый корпус и гиперпараметры, о которых поговорим позднее. Далее названия гиперпараметров и их дефолтные значения будут указаны в соответсвии с реализацией Word2Vec на open-source платформе Gensim.\n",
    "\n",
    "Суть алгоритма заключается в том, что он каждому слову сопоставляет числовой вектор определенной длины таким образом, чтобы близкие слова соответствуют близким векторам. Мерой близости слов выступает их контекстная близость т.е. близкие слова встречаются в тексте рядом с одинаковыми словами. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например слова \"девушка\" и \"женщина\" будут часто употреблятся со словами: \"красивая\", \"прекрасная\", \"чудесная\", \"заботливая\", а слова \"мужчина\" и \"парень\" будут часто употреблятся со словами: \"храбрый\", \"сильный\", \"смелый\", \"могучий\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=download&id=0BzaVUd-GlV7lYXF0b29VRzEwaUU\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Расстоянием между векторами измеряется при помощи [косинусного сходства](https://ru.wikipedia.org/wiki/%D0%92%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BD%D0%B0%D1%8F_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C) (cosine similarity). Косинусное сходство - это мера сходства между двумя векторами предгильбертового пространства, которая используется для измерения косинуса угла между ними. Косинусная мера между векторами x и y длины n вычисляется по формуле:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$cos(\\theta)=\\frac{(x,y)}{|x| |y|}=\\frac{\\sum\\limits_{i=1}^{n}x_i y_i}{\\sqrt{\\sum\\limits_{i=1}^{n}x_i^2} \\sqrt{\\sum\\limits_{i=1}^{n}y_i^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаясь, Word2Vec максимизирует косинусную меру близости между векторами слов, которые встречаются в похожих контекстах и минимизирует косинусную меру между словами, которые не встречаются рядом.Word2Vec получает на вход слово, а на выход передает координаты вектора, соответствующие данному слову.\n",
    "\n",
    "Полученные вектора можно складывать или вычитать друг из друга, сохраняя семантические связи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?export=download&id=0BzaVUd-GlV7lVEFPakZIcGZWVzg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Некоторый недостаток заключается в том, что мы не имеем понятия за что отвечают полученные признаки. Т.е. у нас нет интерпретации, как на картинке ниже. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=download&id=0BzaVUd-GlV7lX1FNTDNxcnFsWnc\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В Word2Vec можно использовать две различных архитектуры нейронной сети с помощью которых осуществляется перевод слова в вектор: \n",
    "\n",
    "- Continuous Bag of Words (CBOW);\n",
    "- Skip-gram.\n",
    "\n",
    "Выбор одной из этих моделей выполняется с помощью гиперпараметра ‘sg’. По умолчанию sg=0 и используется модель CBOW, при sg=1 используется модель Skip-gram.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другим гиперпараметром является размер окна, в котором рассматривается контекст данного слова. В данной реализации используется параметр ‘window’, который определяет максимальное количество слов между данным словом и соседним внутри предложения. Слова стоящие от данного дальше этого значения не будут рассматриваться как его контекст. Какое слово стоит в тексте ближе , а какое дальше от данного слова, не учитывается, при условии, что оба слова попали в окно. Вот пример с параметром window = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=download&id=0BzaVUd-GlV7lMXd0bjRDLUdLN2c\"/>\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=download&id=1c50NoJDfmB_88WxYNy-lDqDLJhz4AwnV\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще один важный гиперпараметр ‘size’ - размерность векторов, соответствующих словам. Если его величина мала, то модель получается грубой и плохо отображает связь между словами внутри данного массива текстов. А при большом значении роль машинного обучения теряется, и сопоставление словам векторов может превратиться в унитарное кодирование слов (one-hot encoding).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо этого есть такие параметры как:\n",
    "\n",
    "alpha - начальный коэффициент скорости обучения (будет линейно падать в ходе обучения).\n",
    "\n",
    "seed = семя для воспроизводимости результатов;\n",
    "\n",
    "min_count = минимальная частота слова, чтобы оно было учтено; \n",
    "\n",
    "max_vocab_size = ограничение на выделение ОЗУ для словаря. 10млн. слов занимают примерно 1GB RAM;\n",
    "\n",
    "workers = число ядер используемых для обучения, повышает скорость;\n",
    "\n",
    "iter = число итераций на каждый текст (по умолчанию 5);\n",
    "\n",
    "sorted_vocab = 1 (значение по умолчанию),то сортирует словарь по убыванию частоты перед назначением слову индекса.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Архитектура нейронной сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь более подробно рассмотрим процесс обучения и архитектуру нейронных сетей.\n",
    "Пусть $V$ - количество слов в словаре, $h$ - размер окна (количество соседних слов, рассматриваемых как контекст данного слова), $N$ - размерность искомых векторов. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec: skip gram & cbow\n",
    "\n",
    "Models __CBOW (Continuous Bag of Words)__ and __Skip gram__ were invented in the now distant 2013,\n",
    "*article*:\n",
    "[*Tomas Mikolov et al.*](https://arxiv.org/pdf/1301.3781v3.pdf)\n",
    "\n",
    "* __CBOW__ Модель предсказывает пропущенное слово, используя контекст (окружающие слова).\n",
    "* __skip gram__ модель обратная к _CBOW_. Она предсказывает контекст на основе выбранного слова.\n",
    "\n",
    "* **Context** фиксированное количество слов слева и справа от выбранного слова (см. рисунок ниже). Длина контекста определяется параметром \"window\".\n",
    "<img src=\"https://drive.google.com/uc?export=download&id=123tOrqr958DAwU60oW4nPccqr7JbnBF7\"/>\n",
    "\n",
    "\n",
    "Сравнение 2ух моделей\n",
    "<img src=\"https://drive.google.com/uc?export=download&id=1AW7mMz3e6AyA0azAwvM40qGE2CnjJ7mS\"/>\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "Существует много реализаций word2vec в т.ч. [gensim](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/word2vec.ipynb).\n",
    "И есть много обученных слов-векторов, которые уже готовы к использованию (Например, GloVe).\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Bag of Words (CBOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда данных мало лучше использовать CBOW. В данном подходе нейронная сеть предсказывает исходное слово по его контексту ($h$ соседним словам). Нейронная сеть состоит из трех слоев: входной, скрытый, выходной.\n",
    "\n",
    "На вход сети подаются $h$ векторов размерности $V$: $x_i=(x_i^1,x_i^2,...,x_i^V), i=1...h$, где $x_i^j=1$, если данное слово является $j$-ым словом из словаря, $x_i^k=0$ для $k\\neq j$. На выходе имеем один вектор размерности $V$: $y=(y^1,y^2,...,y^V)$. На обучающейся выборке $y^j=1$, если предсказываемое слово является $j$-ым словом из словаря, $y^k=0$ для $k\\neq j$. То есть нейронная сеть имеет $h\\times V$ нейронов на входном слое и $V$ нейронов на выходном слое.\n",
    "\n",
    "На скрытом слое сети $N$ нейронов. Именно с помощью весов, расставленных перед нейронами этого слоя, мы получим координаты векторных представлений слов. Функция активации на скрытом уровне — линейная, на выходном уровне — софтмакс (softmax).\n",
    "\n",
    "Сначала рассмотрим простейший случай, когда  $h=1$.То есть будем предсказывать слово $y$ только по одному его соседу $x$. Тогда архитектура сети примет вид, изображенный на схеме:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=download&id=0BzaVUd-GlV7lTUlzUENwVEl3azg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обозначим веса между входным и скрытым уровнями за $W$ - матрица размерности $V\\times N$. Учитывая, что функция активации на скрытом слое линейная, вектор выходов размерности $N$ на скрытом уровне имеет вид: $$v=Wx$$\n",
    "\n",
    "Обозначим веса между скрытым и выходным уровнями за $W'$ - матрица размерности $N\\times V$. Пусть $w'_j$ - $j$-ая строка матрицы $W'$. Тогда входной сигнал $j$-ого нейрона на выходном уровне имеет вид: $$u^j=w'_j v$$\n",
    "\n",
    "Так как на выходном уровне используется функция активации softmax, выходной сигнал на  $j$-ом нейроне выходного слоя принимает вид:\n",
    "\n",
    "$$y^j=\\frac{exp(u^j)}{\\sum\\limits_{k=1}^{V} exp(u^k)}$$\n",
    "\n",
    "Такое представление имеет вероятностную интерпретацию. Пусть $w_I$ - входное слово, а $w_O$ - выходное слово. Обозначим получившееся выражение для $y^j$ за $p(w_j|w_I)$. Обучаясь, сеть максимизирует $y^{j*}=\\frac{exp(u^{j*})}{\\sum\\limits_{k=1}^{V} exp(u^k)}=p(w_O|w_I)$, при условии что $w_O = w_{j*}$.\n",
    "\n",
    "Сеть обучается методом обратного распространения ошибки (backpropagation), то есть сначала корректируются веса $W'$, а затем $W$. Минимизируемый функционал потерь имеет вид:\n",
    "$$L=-log \\ p(w_O|w_I)$$\n",
    "\n",
    "После того, как процесс обучения завершился, $V$ строк длины $N$ матрицы $W'$ дадут нам  координаты векторов, представляющих слова из словаря.\n",
    "\n",
    "Теперь рассмотрим общий случай для произвольного $h$. То есть будем предсказывать слово $y$ только по соседним словам $x_i, i=1...h$. Тогда архитектура сети примет вид, изображенный на схеме:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=download&id=0BzaVUd-GlV7lSjdfLS0yWnlyeFU\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Обозначим веса между входными и скрытым уровнями за $W$ - матрица размерности $Vh\\times N$. Учитывая, что функция активации на скрытом слое линейная, вектор выходов размерности $N$ на скрытом уровне имеет вид: $$v= \\frac{1}{h} W (x_1+...+x_h)$$\n",
    "\n",
    "Далее, аналогично случаю $h=1$ получаем входной сигнал $j$ -ого нейрона на выходном слое сети $$u^j=w'_j v,$$\n",
    "где $w'_j$ - $j$-ая строка матрицы весов между скрытым и выходным уровнями $W'$.\n",
    "\n",
    "Пусть входные слова (соседние слова предсказываемого) - $w_{I,1},...,w_{I,h}$. Тогда выходной сигнал на  $j$-ом нейроне выходного слоя принимает вид:\n",
    "\n",
    "$$y^j=\\frac{exp(u^j)}{\\sum\\limits_{k=1}^{V} exp(u^k)}=p(w_j|w_{I,1},...,w_{I,h})$$\n",
    "\n",
    "Как и ранее, обучаясь, сеть максимизирует $y^{j*}=\\frac{exp(u^{j*})}{\\sum\\limits_{k=1}^{V} exp(u^k)}=p(w_O= w_{j*}|w_{I,1},...,w_{I,h})$, при условии что $w_O = w_{j*}$.\n",
    "\n",
    "Минимизируемый функционал потерь имеет вид:\n",
    "$$L=-log \\ p(w_O|w_{I,1},...,w_{I,h})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае когда данных много, то лучше использовать Skip-Gram. Данный подход обратен CBOW: по заданному слову предсказывается его контекст ($h$ соседних слов). Сеть также состоит из трех слоев: входной, скрытый, выходной.\n",
    "\n",
    "На вход сети подается вектор размерности $V$: $x=(x^1,x^2,...,x^V)$, где $x^j=1$, если данное слово является $j$-ым словом из словаря, $x^k=0$ для $k\\neq j$. На выходе имеем $h$ векторов размерности $V$: $y_i=(y_i^1,y_i^2,...,y_i^V), i=1...h$. На обучающейся выборке $y_i^j=1$, если предсказываемое $i$-е слово из окна является $j$-ым словом из словаря, $y_i^k=0$ для $k\\neq j$. То есть нейронная сеть имеет $V$ нейронов на входном слое и $h\\times V$ нейронов на выходном слое.\n",
    "\n",
    "На скрытом слое сети, как и ранее, $N$ нейронов, а функции активации на скрытом уровне — линейная, на выходном уровне — софтмакс (softmax).\n",
    "\n",
    "Схема архитектуры сети изображена на схеме:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=download&id=0BzaVUd-GlV7lSzRBTXo1d0djM3M\"/>\n",
    "\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=download&id=1sAeYWyR_sMzgHQCAaHH1i9-oY55zSOCs\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обозначим веса между входным и скрытым уровнем за $W$ - матрица размерности $V\\times N$. Учитывая, что функция активации на скрытом слое линейная, вектор выходов размерности $N$ на скрытом уровне имеет вид: $$v=Wx.$$\n",
    "\n",
    "Обозначим веса между скрытым и выходными уровнями за $W'$ - матрица размерности $N \\times hV$. Пусть $w'_j$ - $j$-ая строка матрицы $W'$. Тогда входной сигнал $(i V+j)$-ого нейрона (соответсвует вероятности того, что $i$-е слово из контекста это $j$ слово из словаря) на выходном уровне имеет вид: $$u^{i,j}=u^j=w'_j v , i=1...h, j=1...V$$\n",
    "\n",
    "$u^{i,j}$ одинаково для всех $i$, так как слова в контексте заданного слова равнозначны, то есть все нейроны имеют одинаковые веса.\n",
    "\n",
    "Пусть $w_I$ - входное слово, а $w_{O,i}$ - $i$-е выходное слово, тогда выходной сигнал на  $(i V+j)$-ом нейроне выходного слоя имеет вид:\n",
    "\n",
    "$$y^j=\\frac{exp(u^j)}{\\sum\\limits_{n=1}^{V} exp(u^{i V+n})}=p(w_{O,i}=w_j|w_I)$$\n",
    "\n",
    "Обучаясь, сеть максимизирует $p(w_{O,1}=w_{j_1},...,w_{O,h}=w_{j_h}|w_I)=\\prod\\limits_{k=1}^{h}p(w_{O,k}=w_{j_k}|w_I)$, при условии что $w_{O,k} = w_{j_k}, k=1...h$.\n",
    "\n",
    "Минимизируемый функционал потерь имеет вид:\n",
    "$$L=-log \\ p(w_{O,1},...,w_{O,h}|w_I)$$\n",
    "\n",
    "Воспользуемся формулой Байеса:\n",
    "$$ p(w_{O,1},...,w_{O,h}|w_I)=\\frac{p(w_I|w_{O,1},...,w_{O,h}) p(w_{O,1},...,w_{O,h})}{p(w_I)}$$\n",
    "Из этого выражения видно, что увеличение  $p(w_{O,1},...,w_{O,h}|w_I)$  влечет увеличение $p(w_I|w_{O,1},...,w_{O,h})$. Это соображение говорит о схожетсти описанных подходов.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Скрытый слой\n",
    "\n",
    "При обучении сети на парах слов входной сигнал представляет собой one-hot вектор, представляющий входное слово, а выходной сигнал обучения также является one-hot вектором, представляющим выходное слово. Но когда вы оцениваете обученную сеть по входному слову, выходной вектор будет фактически распределением вероятности (то есть набором значений с плавающей запятой, а не one-hot вектором).\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=download&id=1xGVPM5IItl1ZyUu-bbkwnWl4sckXFLCc\"/>\n",
    "\n",
    "\n",
    "Если вы умножите один one-hot вектор размера 1 x 10 000 на матрицу размера 10 000 x 300, вы получите строку матрицы, соответствующую «1» в вашем one-hot векторе. Вот небольшой пример для наглядности:\n",
    "\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=download&id=1a8H5wLavoZxMx-eg0NgtLML84WzK0J8z\"/>\n",
    "\n",
    "Это означает, что скрытый слой этой модели в действительности работает как справочная/переводная таблица. Выход скрытого слоя - это просто «вектор слов» для входного слова.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выходной слой\n",
    "\n",
    "Например, вектор слова 1 x 300 для «ants» затем подается на выходной слой сети. Выходной слой является классификатором регрессии softmax (softmax regression classifier). Суть его в том, что каждый выходной нейрон (по одному на слово в нашем словаре!) Будет производить выход в диапазоне от 0 до 1, а сумма всех этих выходных значений в сумме даст 1.\n",
    "\n",
    "В частности, каждый выходной нейрон имеет весовой вектор, который он умножает на слово вектор из скрытого слоя, затем он применяет функцию exp (x) к результату. Наконец, чтобы получить выходные данные для суммирования до 1, мы делим этот результат на сумму результатов по всем 10 000 выходным узлам.\n",
    "\n",
    "Вот иллюстрация расчета выхода последнего нейрона для слова «car»\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=download&id=1KMzQ52_ThGj4wS_31sJrxMMYro27gRSf\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Уменьшение вычислительной сложности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Несложно догадаться, что без дополнительных доработок алгоритм будет работать очень медленно,  так как размерность словаря $V$ может достигать очень больших значений (нескольких сотен тысяч). Так как сеть обучается методом обратного распространения ошибки, необходимо вычислять градиент на двух шагах, что очень затратно вычислительно.\n",
    "\n",
    "Рассмотрим два метода решения этой проблемы: иерархический софтмакс (Hierarchical Softmax) и Negative Sampling. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот метод помогает эффективно вычислить значение softmax, используя бинарное дерево. По всем словам в словаре строится дерево Хаффмана. В полученном дереве $V$ слов располагаются на листьях дерева.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=download&id=0BzaVUd-GlV7lNXJ6YWVBM2ZqOWc\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На рисунке изображен пример бинарного дерева. Жирным выделен путь от корня до слова $w_2$. Длину пути обозначим $L(w)$, а $j$-ую вершину на пути к слову $w$ обозначим через $n(w,j)$. Можно доказать, что внутренних вершин (не листьев) $V − 1$.\n",
    "\n",
    "С помощью иерархического softmax вектора $v_{n(w,j)}$ предсказывается для $V-1$ внутренних вершин. А вероятность того, что слово $w$ будет выходным словом (в зависимости от того, что мы предсказываем: слово из контекста или заданное слово по контексту) вычисляется по формуле:\n",
    "$$p(w=w_o)=\\prod_{j=1}^{L(w)-1}\\sigma([n(w,j+1)=lch(n(w,j))] v_{n(w,j)}^T u)$$\n",
    "где $\\sigma()$ - функция softmax; $[true]=1,[false]=-1$; $lch(n)$ - левый сын вершины $n$; $u=v_{w_I}$, если используется метод skip-gram, $u=\\frac{1}{h} \\sum\\limits_{k=1}^{h} v_{w_{I,k}}$, если используется CBOW.\n",
    "\n",
    "Формула можно интуитивно понять, представив, что на каждом шаге мы можем пойти налево или направо с веротяностями:\n",
    "$$p(n,left)=\\sigma(v_n^T u)$$\n",
    "$$p(n,right)=1-p(n,left)=1-\\sigma(v_n^T u)=\\sigma(-v_n^T u)$$\n",
    "Затем на каждм шаге вероятности перемножаются ($L(w)-1$ шагов) и получается искомая формула.\n",
    "\n",
    "При использовании простого softmax для подсчета вероятности слова, приходилось вычислять номирующую сумму по всем словам из словаря, требовалось $O(V)$ операций. Теперь же вероятность слова можно вычислить при помощи последовательных вычислений, которые требуют $O(log(V))$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея этого метода состоит в том, чтобы пересчитывать функционал потерь не по всем словам из словаря, а только по меньшему количеству слов. Тем самым пересчитываться будут вектора не всех слов из словаря, а только некоторого подмножества.\n",
    "\n",
    "Для реализации данной идеи функционал потерь был изменен следующим образом: выходной вектор $w_O$ остается и обновляется, также нужно случайно выбрать  $K$ векторов слов (negative samples), которые не являются подходящими нам (не из контекста входного слова или входных слов). Предполагается, что модель устойчива к шуму и что negative samples имеют некоторое распределение $P_n (w)$, вид распределения задается произвольно.\n",
    "\n",
    "Функция потерь принимает вид:\n",
    "$$L=- log\\sigma(v_{w_O}^T u)-\\sum_{w \\in W_{neg}} log\\sigma(-v_{w}^T u)$$\n",
    "где $W_{neg}=\\{w_j|j=1,...,K\\}$ - $K$ случайно выбранных из словаря слов согласно распределению $P_n (w)$; $u=v_{w_I}$, если используется метод skip-gram, $u=\\frac{1}{h} \\sum\\limits_{k=1}^{h} v_{w_{I,k}}$, если используется CBOW.\n",
    "\n",
    "После нексольких случайных генераций слов negative samples алгоритм сходится."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если параметр модели hs = 1, то будет использован hierarchical softmax . Если hs = 0 (по умолчанию),то будет использован negative sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Применим на наборе данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если вы не имеете достаточно много данных, то ваша модель может ничего полезного не выучить. Поэтому иногда имеет смысл воспользоваться уже предобученной моделью, например на wiki или новостях. Скачать можно например c [code.google](https://code.google.com/archive/p/word2vec/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но в нашем случае необходимо учитывать особенности последовательностей сайтов. Поэтому предобученный Word2Vec нам не поможет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузим библиотеки и установим опции\n",
    "from __future__ import division, print_function\n",
    "# отключим всякие предупреждения Anaconda\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "скачать данные можно со страницы соревнования [\"Catch Me If You Can\"](https://inclass.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>...</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>21668</td>\n",
       "      <td>21669</td>\n",
       "      <td>56</td>\n",
       "      <td>2013-01-12 08:05:57</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2013-01-12 08:05:57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54842</td>\n",
       "      <td>54843</td>\n",
       "      <td>56</td>\n",
       "      <td>2013-01-12 08:37:23</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2013-01-12 08:37:23</td>\n",
       "      <td>56.0</td>\n",
       "      <td>2013-01-12 09:07:07</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2013-01-12 09:07:09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77291</td>\n",
       "      <td>77292</td>\n",
       "      <td>946</td>\n",
       "      <td>2013-01-12 08:50:13</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:14</td>\n",
       "      <td>951.0</td>\n",
       "      <td>2013-01-12 08:50:15</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:15</td>\n",
       "      <td>946.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>948.0</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>784.0</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>949.0</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114020</td>\n",
       "      <td>114021</td>\n",
       "      <td>945</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>948.0</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>949.0</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>948.0</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>945.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>947.0</td>\n",
       "      <td>2013-01-12 08:50:19</td>\n",
       "      <td>945.0</td>\n",
       "      <td>2013-01-12 08:50:19</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:19</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146669</td>\n",
       "      <td>146670</td>\n",
       "      <td>947</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>950.0</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>948.0</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>947.0</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>950.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>951.0</td>\n",
       "      <td>2013-01-12 08:50:22</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:22</td>\n",
       "      <td>947.0</td>\n",
       "      <td>2013-01-12 08:50:22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        session_id  site1               time1  site2               time2  \\\n",
       "21668        21669     56 2013-01-12 08:05:57   55.0 2013-01-12 08:05:57   \n",
       "54842        54843     56 2013-01-12 08:37:23   55.0 2013-01-12 08:37:23   \n",
       "77291        77292    946 2013-01-12 08:50:13  946.0 2013-01-12 08:50:14   \n",
       "114020      114021    945 2013-01-12 08:50:17  948.0 2013-01-12 08:50:17   \n",
       "146669      146670    947 2013-01-12 08:50:20  950.0 2013-01-12 08:50:20   \n",
       "\n",
       "        site3               time3  site4               time4  site5  ...  \\\n",
       "21668     NaN                 NaT    NaN                 NaT    NaN  ...   \n",
       "54842    56.0 2013-01-12 09:07:07   55.0 2013-01-12 09:07:09    NaN  ...   \n",
       "77291   951.0 2013-01-12 08:50:15  946.0 2013-01-12 08:50:15  946.0  ...   \n",
       "114020  949.0 2013-01-12 08:50:18  948.0 2013-01-12 08:50:18  945.0  ...   \n",
       "146669  948.0 2013-01-12 08:50:20  947.0 2013-01-12 08:50:21  950.0  ...   \n",
       "\n",
       "                     time6  site7               time7  site8  \\\n",
       "21668                  NaT    NaN                 NaT    NaN   \n",
       "54842                  NaT    NaN                 NaT    NaN   \n",
       "77291  2013-01-12 08:50:16  948.0 2013-01-12 08:50:16  784.0   \n",
       "114020 2013-01-12 08:50:18  947.0 2013-01-12 08:50:19  945.0   \n",
       "146669 2013-01-12 08:50:21  946.0 2013-01-12 08:50:21  951.0   \n",
       "\n",
       "                     time8  site9               time9  site10  \\\n",
       "21668                  NaT    NaN                 NaT     NaN   \n",
       "54842                  NaT    NaN                 NaT     NaN   \n",
       "77291  2013-01-12 08:50:16  949.0 2013-01-12 08:50:17   946.0   \n",
       "114020 2013-01-12 08:50:19  946.0 2013-01-12 08:50:19   946.0   \n",
       "146669 2013-01-12 08:50:22  946.0 2013-01-12 08:50:22   947.0   \n",
       "\n",
       "                    time10  target  \n",
       "21668                  NaT       0  \n",
       "54842                  NaT       0  \n",
       "77291  2013-01-12 08:50:17       0  \n",
       "114020 2013-01-12 08:50:20       0  \n",
       "146669 2013-01-12 08:50:22       0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# загрузим обучающую и тестовую выборки\n",
    "train_df = pd.read_csv('data/train_sessions.csv')#,index_col='session_id')\n",
    "test_df = pd.read_csv('data/test_sessions.csv')#, index_col='session_id')\n",
    "\n",
    "# приведем колонки time1, ..., time10 к временному формату\n",
    "times = ['time%s' % i for i in range(1, 11)]\n",
    "train_df[times] = train_df[times].apply(pd.to_datetime)\n",
    "test_df[times] = test_df[times].apply(pd.to_datetime)\n",
    "\n",
    "# отсортируем данные по времени\n",
    "train_df = train_df.sort_values(by='time1')\n",
    "\n",
    "# посмотрим на заголовок обучающей выборки\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = ['site%s' % i for i in range(1, 11)]\n",
    "#заменим nan на 0\n",
    "train_df[sites] = train_df[sites].fillna(0).astype('int').astype('str')\n",
    "test_df[sites] = test_df[sites].fillna(0).astype('int').astype('str')\n",
    "#создадим тексты необходимые для обучения word2vec\n",
    "train_df['list'] = train_df['site1']\n",
    "test_df['list'] = test_df['site1']\n",
    "for s in sites[1:]:\n",
    "    train_df['list'] = train_df['list']+\",\"+train_df[s]\n",
    "    test_df['list'] = test_df['list']+\",\"+test_df[s]\n",
    "train_df['list_w'] = train_df['list'].apply(lambda x: x.split(','))\n",
    "test_df['list_w'] = test_df['list'].apply(lambda x: x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['229', '1500', '33', '1500', '391', '35', '29', '2276', '40305', '23']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#В нашем случае предложение это набор сайтов, которые посещал пользователь\n",
    "#нам необязательно переводить цифры в названия сайтов, т.к. алгоритм будем выявлять взаимоствязь их друг с другом\n",
    "train_df['list_w'][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implement word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# подключим word2vec\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#объединим обучающую и тестовую выборки и обучим нашу модель на всех данных \n",
    "#с размером окна в 6=3*2(длина предложения 10 слов) и итоговыми векторами размерности 300, параметр workers отвечает за колчество ядер\n",
    "test_df['target'] = -1\n",
    "data = pd.concat([train_df,test_df],axis=0)\n",
    "\n",
    "model = word2vec.Word2Vec(data['list_w'], size=300, window=3, workers=4) #you can define sg=1 for Skip Gram\n",
    "#создадим словарь со словами и соответсвующими им векторами\n",
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Т.к. сейчас мы каждому слову сопоставили вектор, то нужно решить что сопоставить целому предложению из слов.\n",
    "Один из возможных вариантов это просто усреднить все слова в предложении и получить некоторый смысл всего предложения (если слова нет в тексте, то берем нулевой вектор)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mean_vectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = len(next(iter(w2v.values())))\n",
    "    \n",
    "    def fit(self, X):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] \n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(253561, 300)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_mean=mean_vectorizer(w2v).fit(train_df['list_w']).transform(train_df['list_w'])\n",
    "data_mean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Т.к. мы получили distributed representation, то никакая фича ничего не значит, а значит лучше всего покажут себя линейные комбинации. Например нейронные сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((202849, 300), (50712, 300), 0.009726446765820882, 0.006389020350212968)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Воспользуемся валидацией\n",
    "def split(train,y,ratio):\n",
    "    idx = round(train.shape[0] * ratio)\n",
    "    return train[:idx, :], train[idx:, :], y[:idx], y[idx:]\n",
    "y = train_df['target']\n",
    "Xtr, Xval, ytr, yval = split(data_mean, y,0.8)\n",
    "Xtr.shape,Xval.shape,ytr.mean(),yval.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# подключим библиотеки keras \n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Input\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#опишем нейронную сеть\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=(Xtr.shape[1])))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.2 s, sys: 5.88 s, total: 55 s\n",
      "Wall time: 29.8 s\n"
     ]
    }
   ],
   "source": [
    "%time history = model.fit(Xtr, np.asarray(ytr), batch_size=128, epochs=10, validation_data=(Xval, np.asarray(yval)), class_weight='auto', verbose=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9197321378693928"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = model.predict(Xval, batch_size=128)\n",
    "roc_auc_score(yval, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили результат сопоставимый с моделью, где используется  logistic regressiom на one-hot-encoding сайтов (roc_auc_score = 0.919524709674). Но при этом мы уменьшили размерность пространства до 300. Значит Word2Vec смог выявить зависимости между сессиями.\n",
    "Посмотрим, что произойдет с деревянным алгоритмом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr = xgb.DMatrix(Xtr, label= ytr,missing = np.nan)\n",
    "dval = xgb.DMatrix(Xval, label= yval,missing = np.nan)\n",
    "watchlist = [(dtr, 'train'), (dval, 'eval')]\n",
    "history = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_depth': 26,\n",
    "    'eta': 0.025,\n",
    "    'nthread': 4,\n",
    "    'gamma' : 1,\n",
    "    'alpha' : 1,\n",
    "    'subsample': 0.85,\n",
    "    'eval_metric': ['auc'],\n",
    "    'objective': 'binary:logistic',\n",
    "    'colsample_bytree': 0.9,\n",
    "    'min_child_weight': 100,\n",
    "    'scale_pos_weight':(1)/y.mean(),\n",
    "    'seed':7\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention!** достаточно долго обучается, лучше использовать collab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.956173\teval-auc:0.83916\n",
      "[20]\ttrain-auc:0.98999\teval-auc:0.910304\n",
      "[40]\ttrain-auc:0.992236\teval-auc:0.915499\n",
      "[60]\ttrain-auc:0.993659\teval-auc:0.918051\n",
      "[80]\ttrain-auc:0.994756\teval-auc:0.920258\n",
      "[100]\ttrain-auc:0.995664\teval-auc:0.921828\n",
      "[120]\ttrain-auc:0.996401\teval-auc:0.922937\n",
      "[140]\ttrain-auc:0.996957\teval-auc:0.92416\n",
      "[160]\ttrain-auc:0.997395\teval-auc:0.924811\n",
      "[180]\ttrain-auc:0.997707\teval-auc:0.924671\n",
      "[199]\ttrain-auc:0.99797\teval-auc:0.925023\n"
     ]
    }
   ],
   "source": [
    "model_new = xgb.train(params, dtr, num_boost_round = 200, evals=watchlist,evals_result=history, verbose_eval=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что алгоритм сильно подстраивается под обучающую выборку, поэтому возможно лучше использовать линейные алгоритмы.\n",
    "Посмотрим, что покажет обычный LogisticRegression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "def get_auc_lr_valid(X, y, C=1, seed=7, ratio = 0.8):\n",
    "    # разделим выборку на обучающую и валидационную\n",
    "    idx = round(X.shape[0] * ratio)\n",
    "    # обучение классификатора\n",
    "    lr = LogisticRegression(C=C, random_state=seed, n_jobs=-1).fit(X[:idx], y[:idx])\n",
    "    # прогноз для валидационной выборки\n",
    "    y_pred = lr.predict_proba(X[idx:, :])[:, 1]\n",
    "    # считаем качество\n",
    "    score = roc_auc_score(y[idx:], y_pred)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9008607097809884"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_auc_lr_valid(data_mean, y, C=1, seed=7, ratio = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат LogisticRegression отличается от NN на 0.02, что достаточно существенно.\n",
    "Попробуем улучшить результаты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь вместо обычного среднего, чтобы учесть частоту с которой слово встречается в тексте, возьмем взвешенное среднее. В качестве весов возьмем idf меру слова.  Idf это инверсия частоты, с которой некоторое слово встречается в других документах. Учёт idf уменьшает вес широкоупотребительных слов и увеличивает вес более уникальных слов, которые могут достаточно точно указать на то к какому классу относится текст. В нашем случае, кому принадлежит последовательность посещенных сайтов.\n",
    "$$idf(w,D)=log \\frac{|D|}{|{\\{d \\in D | w \\in d\\}}|}$$\n",
    "где $|D|$ - общее число документов, $\\{d \\in D | w \\in d\\}$ - число документов из $D$, в которых встречается слово $w$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#пропишем класс выполняющий tfidf преобразование.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "\n",
    "class tfidf_vectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = len(next(iter(w2v.values())))\n",
    "\n",
    "    def fit(self, X):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.3 s, sys: 1.07 s, total: 26.3 s\n",
      "Wall time: 27.7 s\n"
     ]
    }
   ],
   "source": [
    "%time data_mean = tfidf_vectorizer(w2v).fit(train_df['list_w']).transform(train_df['list_w'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим изменилось ли качество LogisticRegression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-217d6d32481a>\u001b[0m in \u001b[0;36mget_auc_lr_valid\u001b[0;34m(X, y, C, seed, ratio)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# обучение классификатора\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# прогноз для валидационной выборки\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1549\u001b[0;31m                 sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1550\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m    919\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         epsilon, sample_weight)\n\u001b[0m\u001b[1;32m    922\u001b[0m     \u001b[0;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%time get_auc_lr_valid(data_mean, y, C=1, seed=7, ratio = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "видим прирост на 0.07 (относительно 0.919), значит скорее всего взвешенное среднее помогает лучше отобразить смысл всего предложения через word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# habr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Испробуем силы алгоритма непосредственно на  текстовых данных статей хабра. Скачать их вы можете с [train](https://yadi.sk/d/hAhCuetI3JPouk),[test](https://yadi.sk/d/mLMZZtN63JPouc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(172913, 13) (5405, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>date</th>\n",
       "      <th>tags</th>\n",
       "      <th>title</th>\n",
       "      <th>hubs_title</th>\n",
       "      <th>description</th>\n",
       "      <th>name</th>\n",
       "      <th>hub</th>\n",
       "      <th>png</th>\n",
       "      <th>nick</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>favs_lognorm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://geektimes.ru/post/21866/</td>\n",
       "      <td>2008-03-17T18:55:00.000Z</td>\n",
       "      <td>['eeepc', 'asus', 'ЭТО', 'эльдорадо', 'ура']</td>\n",
       "      <td>eeePC в продаже. Да. Правда.</td>\n",
       "      <td>Железо</td>\n",
       "      <td>Итак, если 3 дня назад я отписался то что в пр...</td>\n",
       "      <td>Сергей 'pokatusher'</td>\n",
       "      <td>hub/hardware</td>\n",
       "      <td>https://habrastorage.org/getpro/habr/olpicture...</td>\n",
       "      <td>@M_org</td>\n",
       "      <td>https://geektimes.ru/users/M_org</td>\n",
       "      <td>Итак, если 3 дня назад я &lt;a href=\"http://habra...</td>\n",
       "      <td>2.484907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://habrahabr.ru/company/aladdinrd/blog/30...</td>\n",
       "      <td>2016-06-24T13:02:00.000Z</td>\n",
       "      <td>['Интеграция', 'шифрование', 'Windows', 'Win32...</td>\n",
       "      <td>«Разрубить Гордиев узел» или преодоление пробл...</td>\n",
       "      <td>Системное программирование</td>\n",
       "      <td>Современная операционная система это сложный и...</td>\n",
       "      <td>Аладдин Р.Д.</td>\n",
       "      <td>hub/system_programming</td>\n",
       "      <td>https://habrastorage.org/files/cbd/cf9/5ff/cbd...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://habrahabr.ru/company/aladdinrd</td>\n",
       "      <td>Современная операционная система это сложный и...</td>\n",
       "      <td>4.174387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://geektimes.ru/post/92887/</td>\n",
       "      <td>2010-05-06T10:00:00.000Z</td>\n",
       "      <td>['mc', 'midnight commander', 'diffview', 'merg...</td>\n",
       "      <td>Релиз Midnight Commander 4.7.2 и 4.7.0.5</td>\n",
       "      <td>Чёрная дыра</td>\n",
       "      <td>Спустя 2 месяца упорных трудов вышла новая вер...</td>\n",
       "      <td>Илья Маслаков</td>\n",
       "      <td>hub/closet</td>\n",
       "      <td>https://geektimes.ru/images/logo.png</td>\n",
       "      <td>@smind</td>\n",
       "      <td>https://geektimes.ru/users/smind</td>\n",
       "      <td>Спустя 2 месяца упорных трудов вышла новая вер...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://habrahabr.ru/post/290824/</td>\n",
       "      <td>2015-05-22T11:01:00.000Z</td>\n",
       "      <td>['бизнес-модель', 'бизнес-моделирование']</td>\n",
       "      <td>7 шагов для постройки правильной бизнес-модели</td>\n",
       "      <td>Интернет-маркетинг</td>\n",
       "      <td>Большинство IT предпринимателей сосредотачиваю...</td>\n",
       "      <td>Александр</td>\n",
       "      <td>hub/internetmarketing</td>\n",
       "      <td>https://habrastorage.org/files/50e/211/9a0/50e...</td>\n",
       "      <td>@jasiejames</td>\n",
       "      <td>https://habrahabr.ru/users/jasiejames</td>\n",
       "      <td>&lt;img src=\"https://habrastorage.org/files/50e/2...</td>\n",
       "      <td>3.496508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://habrahabr.ru/post/190088/</td>\n",
       "      <td>2014-09-04T00:32:00.000Z</td>\n",
       "      <td>['python', 'flask', 'mongodb', 'pet-project']</td>\n",
       "      <td>Thunderargs: практика использования. Часть 2</td>\n",
       "      <td>Программирование</td>\n",
       "      <td>История создания Часть 1  Добрый день. Вкратце...</td>\n",
       "      <td>Данияр Супиев</td>\n",
       "      <td>hub/programming</td>\n",
       "      <td>https://habrahabr.ru/i/habralogo.jpg</td>\n",
       "      <td>@uthunderbird</td>\n",
       "      <td>https://habrahabr.ru/users/uthunderbird</td>\n",
       "      <td>&lt;a href=\"http://habrahabr.ru/post/223041/\"&gt;Ист...</td>\n",
       "      <td>3.688879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 _id  \\\n",
       "0                   https://geektimes.ru/post/21866/   \n",
       "1  https://habrahabr.ru/company/aladdinrd/blog/30...   \n",
       "2                   https://geektimes.ru/post/92887/   \n",
       "3                  https://habrahabr.ru/post/290824/   \n",
       "4                  https://habrahabr.ru/post/190088/   \n",
       "\n",
       "                       date  \\\n",
       "0  2008-03-17T18:55:00.000Z   \n",
       "1  2016-06-24T13:02:00.000Z   \n",
       "2  2010-05-06T10:00:00.000Z   \n",
       "3  2015-05-22T11:01:00.000Z   \n",
       "4  2014-09-04T00:32:00.000Z   \n",
       "\n",
       "                                                tags  \\\n",
       "0       ['eeepc', 'asus', 'ЭТО', 'эльдорадо', 'ура']   \n",
       "1  ['Интеграция', 'шифрование', 'Windows', 'Win32...   \n",
       "2  ['mc', 'midnight commander', 'diffview', 'merg...   \n",
       "3          ['бизнес-модель', 'бизнес-моделирование']   \n",
       "4      ['python', 'flask', 'mongodb', 'pet-project']   \n",
       "\n",
       "                                               title  \\\n",
       "0                       eeePC в продаже. Да. Правда.   \n",
       "1  «Разрубить Гордиев узел» или преодоление пробл...   \n",
       "2           Релиз Midnight Commander 4.7.2 и 4.7.0.5   \n",
       "3     7 шагов для постройки правильной бизнес-модели   \n",
       "4       Thunderargs: практика использования. Часть 2   \n",
       "\n",
       "                   hubs_title  \\\n",
       "0                      Железо   \n",
       "1  Системное программирование   \n",
       "2                 Чёрная дыра   \n",
       "3          Интернет-маркетинг   \n",
       "4            Программирование   \n",
       "\n",
       "                                         description                 name  \\\n",
       "0  Итак, если 3 дня назад я отписался то что в пр...  Сергей 'pokatusher'   \n",
       "1  Современная операционная система это сложный и...         Аладдин Р.Д.   \n",
       "2  Спустя 2 месяца упорных трудов вышла новая вер...        Илья Маслаков   \n",
       "3  Большинство IT предпринимателей сосредотачиваю...            Александр   \n",
       "4  История создания Часть 1  Добрый день. Вкратце...        Данияр Супиев   \n",
       "\n",
       "                      hub                                                png  \\\n",
       "0            hub/hardware  https://habrastorage.org/getpro/habr/olpicture...   \n",
       "1  hub/system_programming  https://habrastorage.org/files/cbd/cf9/5ff/cbd...   \n",
       "2              hub/closet               https://geektimes.ru/images/logo.png   \n",
       "3   hub/internetmarketing  https://habrastorage.org/files/50e/211/9a0/50e...   \n",
       "4         hub/programming               https://habrahabr.ru/i/habralogo.jpg   \n",
       "\n",
       "            nick                                      url  \\\n",
       "0         @M_org         https://geektimes.ru/users/M_org   \n",
       "1            NaN   https://habrahabr.ru/company/aladdinrd   \n",
       "2         @smind         https://geektimes.ru/users/smind   \n",
       "3    @jasiejames    https://habrahabr.ru/users/jasiejames   \n",
       "4  @uthunderbird  https://habrahabr.ru/users/uthunderbird   \n",
       "\n",
       "                                             content  favs_lognorm  \n",
       "0  Итак, если 3 дня назад я <a href=\"http://habra...      2.484907  \n",
       "1  Современная операционная система это сложный и...      4.174387  \n",
       "2  Спустя 2 месяца упорных трудов вышла новая вер...      0.000000  \n",
       "3  <img src=\"https://habrastorage.org/files/50e/2...      3.496508  \n",
       "4  <a href=\"http://habrahabr.ru/post/223041/\">Ист...      3.688879  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain = pd.read_csv('data/train_content.csv')\n",
    "Xtest = pd.read_csv('data/test_content.csv')\n",
    "print(Xtrain.shape,Xtest.shape)\n",
    "Xtrain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем обучать модель на всем содержании статьи. Для этого совершим некоторые преобразования над текстом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию, которая будет преобразовывать тестовую статью в лист из слов необходимый для обучения Word2Vec.\n",
    "Функция получает строку, в которой содержится весь текстовый документ.\n",
    "\n",
    "1)Сначала функция будет удалять все символы кроме букв верхнего и нижнего регистра;\n",
    "\n",
    "2)Затем преобразовывает слова к нижнему регистру;\n",
    "\n",
    "3)После чего удаляет стоп слова из текста, т.к. они не несут никакой информации о содержании;\n",
    "\n",
    "4)Лемматизация, процесс приведения словоформы к лемме — её нормальной (словарной) форме.\n",
    "\n",
    "Функция возвращает лист из слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#подключим необходимые библиотеки\n",
    "#\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "stops = set(stopwords.words(\"english\")) | set(stopwords.words(\"russian\"))\n",
    "def review_to_wordlist(review):\n",
    "    #1)\n",
    "    review_text = re.sub(\"[^а-яА-Яa-zA-Z]\",\" \", review)\n",
    "    #2)\n",
    "    words = review_text.lower().split()\n",
    "    #3)\n",
    "    words = [w for w in words if not w in stops]\n",
    "    #4)\n",
    "    words = [morph.parse(w)[0].normal_form for w in words ]\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лемматизация занимает много врмени, поэтому ее можно убрать в целях более быстрых подсчетов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Преобразуем время\n",
    "Xtrain['date'] = Xtrain['date'].apply(pd.to_datetime)\n",
    "Xtrain['year'] = Xtrain['date'].apply(lambda x: x.year)\n",
    "Xtrain['month'] = Xtrain['date'].apply(lambda x: x.month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем обучаться на 2015 году, а валидироваться на первым 4 месяцам 2016, т.к. в нашей тестовой выборке представлены данные за первые 4 месяца 2017 года. Более правдивую валидацию можно сделать идя по годам увеличивая нашу обучающую выборку и смотря качество на первых четырех месяцах следующего года"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((23425, 15), (7556, 15), 3.4046228249071526, 3.304679829935242)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr = Xtrain[Xtrain['year']==2015]\n",
    "Xval = Xtrain[(Xtrain['year']==2016)& (Xtrain['month']<=4)]\n",
    "ytr = Xtr['favs_lognorm']\n",
    "yval = Xval['favs_lognorm']\n",
    "Xtr.shape,Xval.shape,ytr.mean(),yval.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.concat([Xtr,Xval],axis = 0,ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#у нас есть nan, поэтому преобразуем их к строке\n",
    "data['content_clear'] = data['content'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16min 25s, sys: 1.96 s, total: 16min 27s\n",
      "Wall time: 16min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data['content_clear'] = data['content_clear'].apply(review_to_wordlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "моя оперативная память закончилась и  нужный формат для обучения word2vec из list превратился в str, чтобы избавится от этой проблемы я воспользуюсь следующей библиотекой,которая превратит строку от списка в список."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 33s, sys: 1.83 s, total: 2min 35s\n",
      "Wall time: 2min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import ast\n",
    "def get_list(x):\n",
    "    return ast.literal_eval(x)\n",
    "data['content_clear'] = data['content_clear'].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19min 56s, sys: 3.48 s, total: 20min\n",
      "Wall time: 5min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = word2vec.Word2Vec(data['content_clear'], size=300, window=10, workers=4)\n",
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим чему выучилась модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('massive', 0.6958945393562317),\n",
       " ('mining', 0.6796239018440247),\n",
       " ('scientist', 0.6742461919784546),\n",
       " ('visualization', 0.6403135061264038),\n",
       " ('centers', 0.6386666297912598),\n",
       " ('big', 0.6237790584564209),\n",
       " ('engineering', 0.6209672689437866),\n",
       " ('structures', 0.609510600566864),\n",
       " ('knowledge', 0.6094595193862915),\n",
       " ('scientists', 0.6050446629524231)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['open', 'data','science','best'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "модель обучилась достаточно неплохо, посмотрим на результаты алгоритмов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.3 s, sys: 312 ms, total: 46.6 s\n",
      "Wall time: 46.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_mean = mean_vectorizer(w2v).fit(data['content_clear']).transform(data['content_clear'])\n",
    "data_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((23425, 300), (7556, 300), 3.4046228249071526, 3.304679829935242)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split(train,y,ratio):\n",
    "    idx = ratio\n",
    "    return train[:idx, :], train[idx:, :], y[:idx], y[idx:]\n",
    "y = data['favs_lognorm']\n",
    "Xtr, Xval, ytr, yval = split(data_mean, y,23425)\n",
    "Xtr.shape,Xval.shape,ytr.mean(),yval.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ошибка на трейне 0.734248488422\n",
      "Ошибка на валидации 0.665592676973\n",
      "Ошибка на валидации предсказываем медиану 1.44601638512\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "model = Ridge(alpha = 1,random_state=7)\n",
    "model.fit(Xtr, ytr)\n",
    "train_preds = model.predict(Xtr)\n",
    "valid_preds = model.predict(Xval)\n",
    "ymed = np.ones(len(valid_preds))*ytr.median()\n",
    "print('Ошибка на трейне',mean_squared_error(ytr, train_preds))\n",
    "print('Ошибка на валидации',mean_squared_error(yval, valid_preds))\n",
    "print('Ошибка на валидации предсказываем медиану',mean_squared_error(yval, ymed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34min 40s, sys: 796 ms, total: 34min 41s\n",
      "Wall time: 34min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_mean_tfidf = tfidf_vectorizer(w2v).fit(data['content_clear']).transform(data['content_clear'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((23425, 300), (7556, 300), 3.4046228249071526, 3.304679829935242)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data['favs_lognorm']\n",
    "Xtr, Xval, ytr, yval = split(data_mean_tfidf, y,23425)\n",
    "Xtr.shape,Xval.shape,ytr.mean(),yval.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ошибка на трейне 0.743623730976\n",
      "Ошибка на валидации 0.675584372744\n",
      "Ошибка на валидации предсказываем медиану 1.44601638512\n"
     ]
    }
   ],
   "source": [
    "model = Ridge(alpha = 1,random_state=7)\n",
    "model.fit(Xtr, ytr)\n",
    "train_preds = model.predict(Xtr)\n",
    "valid_preds = model.predict(Xval)\n",
    "ymed = np.ones(len(valid_preds))*ytr.median()\n",
    "print('Ошибка на трейне',mean_squared_error(ytr, train_preds))\n",
    "print('Ошибка на валидации',mean_squared_error(yval, valid_preds))\n",
    "print('Ошибка на валидации предсказываем медиану',mean_squared_error(yval, ymed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем нейронные сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# подключим библиотеки keras \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import regularizers\n",
    "from keras.wrappers.scikit_learn import KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=Xtr.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "estimator = KerasRegressor(build_fn=baseline_model,epochs=20, nb_epoch=20, batch_size=64,validation_data=(Xval, yval), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23425 samples, validate on 7556 samples\n",
      "Epoch 1/20\n",
      "1s - loss: 1.7292 - val_loss: 0.7336\n",
      "Epoch 2/20\n",
      "0s - loss: 1.2382 - val_loss: 0.6738\n",
      "Epoch 3/20\n",
      "0s - loss: 1.1379 - val_loss: 0.6916\n",
      "Epoch 4/20\n",
      "0s - loss: 1.0785 - val_loss: 0.6963\n",
      "Epoch 5/20\n",
      "0s - loss: 1.0362 - val_loss: 0.6256\n",
      "Epoch 6/20\n",
      "0s - loss: 0.9858 - val_loss: 0.6393\n",
      "Epoch 7/20\n",
      "0s - loss: 0.9508 - val_loss: 0.6424\n",
      "Epoch 8/20\n",
      "0s - loss: 0.9066 - val_loss: 0.6231\n",
      "Epoch 9/20\n",
      "0s - loss: 0.8819 - val_loss: 0.6207\n",
      "Epoch 10/20\n",
      "0s - loss: 0.8634 - val_loss: 0.5993\n",
      "Epoch 11/20\n",
      "1s - loss: 0.8401 - val_loss: 0.6093\n",
      "Epoch 12/20\n",
      "1s - loss: 0.8152 - val_loss: 0.6006\n",
      "Epoch 13/20\n",
      "0s - loss: 0.8005 - val_loss: 0.5931\n",
      "Epoch 14/20\n",
      "0s - loss: 0.7736 - val_loss: 0.6245\n",
      "Epoch 15/20\n",
      "0s - loss: 0.7599 - val_loss: 0.5978\n",
      "Epoch 16/20\n",
      "1s - loss: 0.7407 - val_loss: 0.6593\n",
      "Epoch 17/20\n",
      "1s - loss: 0.7339 - val_loss: 0.5906\n",
      "Epoch 18/20\n",
      "1s - loss: 0.7256 - val_loss: 0.5878\n",
      "Epoch 19/20\n",
      "1s - loss: 0.7117 - val_loss: 0.6123\n",
      "Epoch 20/20\n",
      "0s - loss: 0.7069 - val_loss: 0.5948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efff19ce7f0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.fit(Xtr, ytr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили более хороший результат по сравнению с ридж регрессией."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Вывод\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы рассмотрели принцип работы Word2Vec и его модификации, реализованные в библиотеки gensim.\n",
    "Применили Word2Vec и поняли, что Word2Vec хороший способ преобразования нечисловых данных в ветора(сохраняя их смысловую близость), на которых уже можно обучать известные вам модели, без сильного увеличения размерности признакового пространства и с сохранением достойного качества.\n",
    "\n",
    "Что можно сделать еще?\n",
    "Ввиду небольшой размерности признакового пространства можно добавить новые признаки основанные на времени, например различные статистические показатели на разницах во времени в переходах между страницами, датой публикации. Можно обучить еще один Word2Vec на тегах или описаниях и добавить, как новые фичи. Можно рассмотреть есть ли картинки в статье, ее длину и прочее.\n",
    "\n",
    "Спасибо за внимание!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Полезные ссылки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- [туториал Bag of Words Meets Bags of Popcorn на kaggle](https://www.kaggle.com/c/word2vec-nlp-tutorial)\n",
    "- [библиотека gensim](https://radimrehurek.com/gensim/index.html)\n",
    "- [Word2Vec Parameter Learning Explained, Xin Rong](https://arxiv.org/pdf/1411.2738.pdf)\n",
    "- [Distributed Representations of Words and Phrases and their Compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "- [Negative-Sampling Word-Embedding Method](https://arxiv.org/pdf/1402.3722.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
