{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis using word and document embeddings. IMDB dataset\n",
    "Ссылка на Google colab:\n",
    "\n",
    "https://drive.google.com/file/d/1ECvHDLo5j27xOEg38RkjfEY6tKym4tUM/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Theoretical part\n",
    "\n",
    "### Word and Document mbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words Model\n",
    "Early state-of-the-art document representations were based on the <a href=\"https://en.wikipedia.org/wiki/Bag-of-words_model\">bag-of-words model</a>, which represent input documents as a fixed-length vector. For example, borrowing from the Wikipedia article, the two documents  \n",
    "(1) `John likes to watch movies. Mary likes movies too.`  \n",
    "(2) `John also likes to watch football games.`  \n",
    "are used to construct a length 10 list of words  \n",
    "`[\"John\", \"likes\", \"to\", \"watch\", \"movies\", \"Mary\", \"too\", \"also\", \"football\", \"games\"]`  \n",
    "so then we can represent the two documents as fixed length vectors whose elements are the frequencies of the corresponding words in our list  \n",
    "(1) `[1, 2, 1, 1, 2, 1, 1, 0, 0, 0]`  \n",
    "(2) `[1, 1, 1, 1, 0, 0, 0, 1, 1, 1]`  \n",
    "Bag-of-words models are surprisingly effective but still lose information about word order. Bag of <a href=\"https://en.wikipedia.org/wiki/N-gram\">n-grams</a> models consider word phrases of length n to represent documents as fixed-length vectors to capture local word order but suffer from data sparsity and high dimensionality.\n",
    "\n",
    "### `Word2Vec`\n",
    "`Word2Vec` is a more recent model that embeds words in a lower-dimensional vector space using a shallow neural network. The result is a set of word-vectors where vectors close together in vector space have similar meanings based on context, and word-vectors distant to each other have differing meanings. For example, `strong` and `powerful` would be close together and `strong` and `Paris` would be relatively far. There are two versions of this model based on skip-grams (SG) and continuous-bag-of-words (CBOW), both implemented by the gensim `Word2Vec` class.\n",
    "\n",
    "\n",
    "#### `Word2Vec` - Skip-gram Model\n",
    "The skip-gram <a href=\"http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\">word2vec</a> model, for example, takes in pairs (word1, word2) generated by moving a window across text data, and trains a 1-hidden-layer neural network based on the synthetic task of given an input word, giving us a predicted probability distribution of nearby words to the input. A virtual <a href=\"https://en.wikipedia.org/wiki/One-hot\">one-hot</a> encoding of words goes through a 'projection layer' to the hidden layer; these projection weights are later interpreted as the word embeddings. So if the hidden layer has 300 neurons, this network will give us 300-dimensional word embeddings.\n",
    "\n",
    "#### `Word2Vec` - Continuous-bag-of-words Model\n",
    "Continuous-bag-of-words Word2vec is very similar to the skip-gram model. It is also a 1-hidden-layer neural network. The synthetic training task now uses the average of multiple input context words, rather than a single word as in skip-gram, to predict the center word. Again, the projection weights that turn one-hot words into averageable vectors, of the same width as the hidden layer, are interpreted as the word embeddings. \n",
    "\n",
    "But, Word2Vec doesn't yet get us fixed-size vectors for longer texts.\n",
    "\n",
    "\n",
    "### Paragraph Vector, aka gensim `Doc2Vec`\n",
    "The straightforward approach of averaging each of a text's words' word-vectors creates a quick and crude document-vector that can often be useful. However, Le and Mikolov in 2014 introduced the <i>Paragraph Vector</i>, which usually outperforms such simple-averaging.\n",
    "\n",
    "The basic idea is: act as if a document has another floating word-like vector, which contributes to all training predictions, and is updated like other word-vectors, but we will call it a doc-vector. Gensim's `Doc2Vec` class implements this algorithm. \n",
    "\n",
    "#### Paragraph Vector - Distributed Memory (PV-DM)\n",
    "This is the Paragraph Vector model analogous to Word2Vec CBOW. The doc-vectors are obtained by training a neural network on the synthetic task of predicting a center word based an average of both context word-vectors and the full document's doc-vector.\n",
    "\n",
    "#### Paragraph Vector - Distributed Bag of Words (PV-DBOW)\n",
    "This is the Paragraph Vector model analogous to Word2Vec SG. The doc-vectors are obtained by training a neural network on the synthetic task of predicting a target word just from the full document's doc-vector. (It is also common to combine this with skip-gram testing, using both the doc-vector and nearby word-vectors to predict a single target word, but only one at a time.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Practical part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Use word embeddings for text classification - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement using a text classifier based on logistic regression where pre-trained word embeddings are used. You need to simply average word embeddings of a sentence (perform average pooling of word vectors) and they apply the logistic regression to the output representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process for using word embeddings as the initial embedding matrix involves first loading the\n",
    "embeddings from the disk, then selecting the correct subset of embeddings for the words that are\n",
    "actually present in the data, and finally setting the Embedding layer’s weight matrix as the loaded\n",
    "subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use ``torch`` use the ``torch.nn.Embedding`` to load pre-trained word embeddings. Use the [GloVe](http://nlp.stanford.edu/data/wordvecs/glove.6B.zip) embeddings. Otherwise you can use ``gensim`` and ``sklearn`` or similar packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statistics\n",
    "from collections import defaultdict\n",
    "import codecs\n",
    "import re\n",
    "import math\n",
    "from math import pi,log, exp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(texts, n):\n",
    "    n_grams = []\n",
    "    for text in texts:\n",
    "        text = text.lower()        \n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)        \n",
    "        tokens = [token for token in text.split(\" \") if token != \"\"]\n",
    "        ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "        n_grams.append([\" \".join(ngram) for ngram in ngrams])\n",
    "    return n_grams\n",
    "\n",
    "def preprocess(texts):\n",
    "    def replacer(check):\n",
    "        if check.group(1) is not None:\n",
    "            return '{} '.format(check.group(1))\n",
    "        else:\n",
    "            return ' {}'.format(check.group(2))\n",
    "\n",
    "    comp = re.compile(r'^(\\W+)|(\\W+)$')\n",
    "    \n",
    "    return [text.split() for text in [\" \".join([comp.sub(replacer, word) for word in i.split()]).lower() for i in texts]]\n",
    "\n",
    "def tokenize(texts):\n",
    "    return [text.split() for text in texts]\n",
    "\n",
    "def read_file(textname):\n",
    "    file = open(textname, 'r')\n",
    "    if file.mode == 'r':\n",
    "        ex_data = file.readlines()\n",
    "        \n",
    "    return [re.sub(\"\\n|\\r''\", \"\", ex_data[i]) for i in range(len(ex_data))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000,) (15000,)\n"
     ]
    }
   ],
   "source": [
    "train_texts = read_file('train.texts')\n",
    "test_texts = read_file('test.texts')\n",
    "train_labels = read_file('train.labels')\n",
    "train_labels_enc = [0 if i=='pos' else 1 for i in train_labels]\n",
    "print(np.asarray(train_texts).shape, np.asarray(train_labels).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If the myth regarding broken mirrors would be accurate, everybody involved in this production would now face approximately 170 years of bad luck, because there are a lot of mirrors falling to little pieces here. If only the script was as shattering as the glass, then \"The Broken\" would have been a brilliant film. Now it\\'s sadly just an overlong, derivative and dull movie with only just a handful of remarkable ideas and memorable sequences. Sean Ellis made a very stylish and elegantly photographed movie, but the story is lackluster and the total absence of logic and explanation is really frustrating. I got into a discussion with a friend regarding the basic concept and \"meaning\" of the film. He thinks Ellis found inspiration in an old legend claiming that spotting your doppelganger is a foreboding of how you\\'re going to die. Interesting theory, but I\\'m not familiar with this legend and couldn\\'t find anything on the Internet about this, neither. Personally, I just think \"The Broken\" is yet another umpteenth variation on the theme of \"Invasion of the Body Snatchers\" but without the alien interference. \"The Broken\" centers on the American McVey family living in London, and particularly Gina. When a mirror spontaneously breaks during a birthday celebration, this triggers a whole series of mysterious and seemingly supernatural events. Gina spots herself driving by in a car and follows her mirror image to an apartment building. Whilst driving home in a state of mental confusion, she causes a terrible car accident and ends up in the hospital. When dismissed, Gina feels like her whole surrounding is changing. She doesn\\'t recognize her own boyfriend anymore and uncanny fragments of the accident keep flashing before her eyes. Does she suffer from mental traumas invoked by the accident or is there really a supernatural conspiracy happening all around her? Writer/director Sean Ellis definitely invokes feelings of curiosity and suspense in his script, but unfortunately he fails to properly elaborate them. \"The Broken\" is a truly atmospheric and stylish effort, but only after just half an hour of film, you come to the painful conclusion it shall just remain a beautiful but empty package. There\\'s a frustratingly high amount of \"fake\" suspense in this film. This means building up tension, through ominous music and eerie camera angels, when absolutely nothing has even happened so far. By the time the actually mysteriousness kicks in, these tricks don\\'t have any scary effect on you anymore. Some of my fellow reviewers around here compare the film and particularly Sean Ellis\\' style with the repertoires of David Lynch, Stanley Kubrick and even Alfred Hitchcock, but that is way, way \\x85  WAY too much honor. PS: what is up with that alternate spelling; the one with the Scandinavian \"ø\"'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 101334)\n",
      "[[ 2 26  1 ...  0  0  0]\n",
      " [ 0  4  0 ...  0  0  0]\n",
      " [ 1  5  0 ...  0  0  0]\n",
      " ...\n",
      " [ 2  5  0 ...  0  0  0]\n",
      " [ 0 16  0 ...  0  0  0]\n",
      " [ 4 36  0 ...  1  1  1]]\n"
     ]
    }
   ],
   "source": [
    "#Create sparse matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "texts = preprocess(train_texts)\n",
    "index1 = [0]\n",
    "index2 = []\n",
    "bow = []\n",
    "vocab = {}\n",
    "for text in texts:\n",
    "    for word in text:\n",
    "        index = vocab.setdefault(word, len(vocab))\n",
    "        index2.append(index)\n",
    "        bow.append(1)\n",
    "    index1.append(len(index2))\n",
    "\n",
    "sparse_matrix = csr_matrix((bow, index2, index1), dtype=int).toarray()\n",
    "print(sparse_matrix.shape)\n",
    "print(sparse_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this matrix $i^{th}$ row contains bag-of-words vector, $j^{th}$ component is the absolute frequency of $j^{th}$ token from the vocabulary in the $i^{th}$ review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try model on our sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = LogisticRegression(max_iter = 300, penalty = 'l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dochkinavika/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.9 s, sys: 19 s, total: 36.9 s\n",
      "Wall time: 39.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=300,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time model2.fit(sparse_matrix, np.asarray(train_labels_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9996666666666667"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model2.predict(sparse_matrix)\n",
    "(preds == train_labels_enc).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['if the',\n",
       " 'the myth',\n",
       " 'myth regarding',\n",
       " 'regarding broken',\n",
       " 'broken mirrors',\n",
       " 'mirrors would',\n",
       " 'would be',\n",
       " 'be accurate',\n",
       " 'accurate everybody',\n",
       " 'everybody involved']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = generate_ngrams(train_texts,2)\n",
    "bigrams[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 963095)\n",
      "[[1 1 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "#Create sparse matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "texts = generate_ngrams(train_texts,2)\n",
    "index1 = [0]\n",
    "index2 = []\n",
    "bow = []\n",
    "vocab = {}\n",
    "for text in texts:\n",
    "    for word in text:\n",
    "        index = vocab.setdefault(word, len(vocab))\n",
    "        index2.append(index)\n",
    "        bow.append(1)\n",
    "    index1.append(len(index2))\n",
    "\n",
    "sparse_matrix_ngr = csr_matrix((bow, index2, index1), dtype=int).toarray()\n",
    "print(sparse_matrix_ngr.shape)\n",
    "print(sparse_matrix_ngr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sparse_matrix_ngr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sparse_matrix_ngr' is not defined"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sparse_matrix_ngr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-466-695d758f64cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model.fit(sparse_matrix_ngr, np.asarray(train_labels_enc))'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_matrix_ngr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#mean accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtrain_labels_enc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sparse_matrix_ngr' is not defined"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=100)\n",
    "%time model.fit(sparse_matrix_ngr, np.asarray(train_labels_enc))\n",
    "preds = model.predict(sparse_matrix_ngr)\n",
    "#mean accuracy\n",
    "(preds == train_labels_enc).mean(dtype=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['if', 'the', 'myth', 'regarding', 'broken', 'mirrors', 'would', 'be', 'accurate,', 'everybody', 'involved', 'in', 'this', 'production', 'would', 'now', 'face', 'approximately', '170', 'years']\n"
     ]
    }
   ],
   "source": [
    "#tokenize and lowercase texts\n",
    "def tokenize(texts):\n",
    "    return [text.lower().split() for text in texts]\n",
    "\n",
    "tokenized_train_data = tokenize(train_texts)\n",
    "print(tokenized_train_data[0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(tokenized_train_data, \n",
    "                 size=32,      # embedding vector size\n",
    "                 min_count=100,  # consider words that occured at least 100 times\n",
    "                 window=5).wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.1927758e+00,  1.4351714e+00,  1.4090706e+00, -9.6739531e-01,\n",
       "        2.2448234e-01, -9.9713367e-01, -4.6309307e-01, -2.3953633e-01,\n",
       "       -1.5805256e-01, -5.0986469e-01, -7.7348888e-01, -5.0096147e-02,\n",
       "        1.4075789e+00,  2.3532798e+00,  6.8942630e-01,  1.2317375e+00,\n",
       "        1.6273854e+00, -1.2622250e+00, -2.4078004e+00, -7.9034513e-01,\n",
       "       -2.0152804e-01,  2.4576237e+00, -1.1414950e+00,  1.4708486e+00,\n",
       "       -4.1453013e+00,  1.6182142e+00,  3.0991731e-03,  4.3842873e-01,\n",
       "       -2.9843295e+00,  1.7097746e+00,  2.4455978e-01, -2.5442025e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_vector('film')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('award', 0.9104125499725342),\n",
       " ('academy', 0.8780422210693359),\n",
       " ('won', 0.8151159286499023),\n",
       " ('nominated', 0.808489203453064),\n",
       " ('actor', 0.7820138335227966),\n",
       " ('director,', 0.7722467184066772),\n",
       " ('winning', 0.7290933132171631),\n",
       " ('awards', 0.7244893312454224),\n",
       " ('bruce', 0.7120931148529053),\n",
       " ('actress', 0.7081336975097656)]"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('oscar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "\n",
    "    def __init__(self, word_model):\n",
    "        self.word_model = word_model\n",
    "        self.vector_size = word_model.wv.vector_size\n",
    "\n",
    "    def fit(self):  \n",
    "        return self\n",
    "\n",
    "    def transform(self, docs): \n",
    "        doc_word_vector = self.word_average_list(docs)\n",
    "        return doc_word_vector\n",
    "    def word_average(self, sent):\n",
    "\n",
    "        mean = []\n",
    "        for word in sent:\n",
    "            if word in self.word_model.wv.vocab:\n",
    "                mean.append(self.word_model.wv.get_vector(word))\n",
    "\n",
    "        if not mean:  # empty words\n",
    "            # If a text is empty, return a vector of zeros.\n",
    "            logging.warning(\"can't compute average\".format(sent))\n",
    "            return np.zeros(self.vector_size)\n",
    "        else:\n",
    "            mean = np.array(mean).mean(axis=0)\n",
    "            return mean\n",
    "\n",
    "\n",
    "    def word_average_list(self, docs):\n",
    "\n",
    "        return np.vstack([self.word_average(sent) for sent in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dochkinavika/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"\n",
      "/Users/dochkinavika/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "/Users/dochkinavika/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    }
   ],
   "source": [
    "mean_vec_tr = MeanEmbeddingVectorizer(model)\n",
    "doc_vec = mean_vec_tr.transform(tokenized_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.06061316, -0.34751496,  0.08287247, -0.16194482,  0.39507714,\n",
       "       -0.04011344,  0.07043894,  0.09254696, -0.23309317, -0.42761686,\n",
       "       -0.3426229 , -0.12471391,  0.00167603,  0.2064087 ,  0.3749557 ,\n",
       "       -0.04829623, -0.18988962, -0.2737884 ,  0.27389583,  0.14256309,\n",
       "       -0.05938748,  0.03134052, -0.00522704, -0.17655511, -0.03553925,\n",
       "        0.2433135 , -0.09339889, -0.10438342, -0.08998757,  0.09295134,\n",
       "       -0.01837852, -0.06334517], dtype=float32)"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 134 ms, sys: 11.2 ms, total: 145 ms\n",
      "Wall time: 150 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dochkinavika/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=300,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check how it works with sklearn\n",
    "%time model2.fit(doc_vec, np.asarray(train_labels_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7615333333333333\n"
     ]
    }
   ],
   "source": [
    "preds = model2.predict(doc_vec)\n",
    "print('accuracy:', (preds == train_labels_enc).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe: Global Vectors for Word Representation\n",
    "\n",
    "### Introduction\n",
    "\n",
    "GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\n",
    "\n",
    "### Differences to Word2Vec:\n",
    "\n",
    "1. Presence of Neural Networks: GloVe does not use neural networks while word2vec does. In GloVe, the loss function is the difference between the product of word embeddings and the log of the probability of co-occurrence. We try to reduce that and use SGD but solve it as we would solve a linear regression. While in the case of word2vec, we either train the word on its context (skip-gram) or train the context on the word (continuous bag of words) using a 1-hidden layer neural network.\n",
    "2. Global information: word2vec does not have any explicit global information embedded in it by default. GloVe creates a global co-occurrence matrix by estimating the probability a given word will co-occur with other words. This presence of global information makes GloVe ideally work better. Although in a practical sense, they work almost similar and people have found similar performance with both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400001, 100)"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_file = datapath('/Users/dochkinavika/Downloads/glove.6B/glove.6B.100d.txt')\n",
    "word2vec_glove_file = get_tmpfile(\"glove.6B.100d.word2vec.txt\")\n",
    "glove2word2vec(glove_file, word2vec_glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dochkinavika/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"\n",
      "/Users/dochkinavika/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "/Users/dochkinavika/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    }
   ],
   "source": [
    "glove_word_model = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
    "glove_mean_vec_tr = MeanEmbeddingVectorizer(glove_word_model)\n",
    "glove_word_vec = glove_mean_vec_tr.transform(tokenized_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 100)"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_word_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dochkinavika/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 309 ms, sys: 12.4 ms, total: 322 ms\n",
      "Wall time: 323 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=300,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check how it works with sklearn logregression\n",
    "%time model2.fit(glove_word_vec, np.asarray(train_labels_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.7899333333333334\n"
     ]
    }
   ],
   "source": [
    "preds = model2.predict(glove_word_vec)\n",
    "print('accuracy: ', (preds == train_labels_enc).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Use word embeddings for text classification - FFNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same pre-trained word embeddings, but use instead of Logistic Regression a feedforward neural network. For both logistic regression and FFNN model, perform tuning of meta-parameters, such as the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_process(X_train, X_test, y_train, y_test):\n",
    "    X_train = torch.from_numpy(X_train)\n",
    "    X_test = torch.from_numpy(X_test)\n",
    "    y_train = torch.from_numpy(np.array(y_train))\n",
    "    y_test = torch.from_numpy(np.array(y_test))\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(glove_word_vec, train_labels_enc, test_size=0.25, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_data_process(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward(torch.nn.Module):\n",
    "        def __init__(self, input_size, hidden_size_in, hidden_size_out):\n",
    "            super(Feedforward, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            self.hidden_size_in  = hidden_size_in\n",
    "            self.hidden_size_out  = hidden_size_out\n",
    "            self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size_in)\n",
    "            self.relu1 = torch.nn.ReLU()\n",
    "            self.fc2 = nn.Linear(self.hidden_size_in, self.hidden_size_out)\n",
    "            self.relu2 = nn.ReLU()\n",
    "            self.fc3 = torch.nn.Linear(self.hidden_size_out, 2)\n",
    "            self.sigmoid = torch.nn.Sigmoid()\n",
    "            \n",
    "        def forward(self, x):\n",
    "            hidden = self.fc1(x)\n",
    "            output = self.relu1(hidden)\n",
    "            output = self.fc2(output)\n",
    "            output = self.fc3(output)\n",
    "            output = self.sigmoid(output)\n",
    "            return output\n",
    "        \n",
    "def train(eval_model, X, y,optimizer, loss_func=nn.CrossEntropyLoss(), num_epoches = 200, loss_print = False):\n",
    "    for i in range(num_epoches):\n",
    "        \n",
    "        eval_model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        y_predicted = eval_model(X)\n",
    "        loss = loss_func(y_predicted, y)\n",
    "        if loss_print == True:\n",
    "            print(i, loss.item())\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "def predict(eval_model, X):\n",
    "    out = eval_model(X)\n",
    "    return torch.max(out, 1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define NN model\n",
    "NN = Feedforward(X_train.shape[1], hidden_size_in = 500, hidden_size_out = 100)\n",
    "optimizer = torch.optim.Adam(NN.parameters(), lr=0.01)\n",
    "\n",
    "train(NN,X_train,y_train,optimizer,num_epoches=200)\n",
    "y_pred = predict(NN,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.7653333333333333\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: ', accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 200d embeddings for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400001, 200)"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_file = datapath('/Users/dochkinavika/Downloads/glove.6B/glove.6B.200d.txt')\n",
    "word2vec_glove_file = get_tmpfile(\"glove.6B.200d.word2vec.txt\")\n",
    "glove2word2vec(glove_file, word2vec_glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dochkinavika/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"\n",
      "/Users/dochkinavika/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "/Users/dochkinavika/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    }
   ],
   "source": [
    "glove_word_model = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
    "glove_mean_vec_tr = MeanEmbeddingVectorizer(glove_word_model)\n",
    "glove_word_vec = glove_mean_vec_tr.transform(tokenized_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 200)"
      ]
     },
     "execution_count": 577,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_word_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(glove_word_vec, train_labels_enc, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_data_process(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learning rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define NN model, learning rate = 0.01\n",
    "NN = Feedforward(X_train.shape[1], hidden_size_in = 500, hidden_size_out = 100)\n",
    "optimizer = torch.optim.Adam(NN.parameters(), lr=0.01)\n",
    "\n",
    "train(NN,X_train,y_train,optimizer,num_epoches=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.8053333333333333\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(NN,X_test)\n",
    "print('Test accuracy: ', accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learning rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define NN model, learning rate = 0.001\n",
    "NN = Feedforward(X_train.shape[1], hidden_size_in = 500, hidden_size_out = 100)\n",
    "optimizer = torch.optim.Adam(NN.parameters(), lr=0.001)\n",
    "\n",
    "train(NN,X_train,y_train,optimizer,num_epoches=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.8103333333333333\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(NN,X_test)\n",
    "print('Test accuracy: ', accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learning rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define NN model, learning rate = 0.0001\n",
    "NN = Feedforward(X_train.shape[1], hidden_size_in = 500, hidden_size_out = 100)\n",
    "optimizer = torch.optim.Adam(NN.parameters(), lr=0.0001)\n",
    "\n",
    "train(NN,X_train,y_train,optimizer,num_epoches=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.8076666666666666\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(NN,X_test)\n",
    "print('Test accuracy: ', accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 200d embeddings for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dochkinavika/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 637 ms, sys: 37.9 ms, total: 675 ms\n",
      "Wall time: 683 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=300,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time model2.fit(glove_word_vec, np.asarray(train_labels_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8183333333333334\n"
     ]
    }
   ],
   "source": [
    "preds = model2.predict(glove_word_vec)\n",
    "print('accuracy: ', (preds == train_labels_enc).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Takeaway: \n",
    "\n",
    "200d embeddings from Glove works a little bit better for NN and LogRegression comparing to 100d\n",
    "\n",
    "leraning rate 0.001 is optimal in both cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Use of document embeddings for text classification \n",
    "\n",
    "Use ``gensim`` to obtain document embeddings for all reviews. Build a model based on logistic regression using ``sklearn`` which load these embeddings for each document and performs a classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocModel(object):\n",
    "\n",
    "    def __init__(self, docs, **kwargs):\n",
    "\n",
    "        self.model = Doc2Vec(**kwargs)\n",
    "        self.docs = docs\n",
    "        self.model.build_vocab([x for x in self.docs])\n",
    "    def custom_train(self, fixed_lr=False, fixed_lr_epochs=None):\n",
    "\n",
    "        if not fixed_lr:\n",
    "            self.model.train([x for x in self.docs],\n",
    "                    total_examples=len(self.docs),\n",
    "                     epochs=self.model.epochs)\n",
    "        else:\n",
    "            for _ in range(fixed_lr_epochs):\n",
    "                self.model.train(utils.shuffle([x for x in self.docs]),\n",
    "                         total_examples=len(self.docs),\n",
    "                         epochs=1)\n",
    "                self.model.alpha -= 0.002\n",
    "                self.model.min_alpha = self.model.alpha  # fixed learning rate\n",
    "\n",
    "\n",
    "    def test_orig_doc_infer(self):\n",
    "\n",
    "        idx = np.random.randint(len(self.docs))\n",
    "        print('idx: ' + str(idx))\n",
    "        doc = [doc for doc in self.docs if doc.tags[0] == idx]\n",
    "        inferred_vec = self.model.infer_vector(doc[0].words)\n",
    "        print(self.model.docvecs.most_similar([inferred_vec]))  # wrap vec in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_args = {\n",
    "    'dm': 1,\n",
    "    'dm_mean': 1,\n",
    "    'vector_size': 100,\n",
    "    'window': 5,\n",
    "    'negative': 5,\n",
    "    'hs': 0,\n",
    "    'min_count': 2,\n",
    "    'sample': 0,\n",
    "    'alpha': 0.025,\n",
    "    'min_alpha': 0.025,\n",
    "    'epochs': 100,\n",
    "    'comment': 'alpha=0.025'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['very', 'intelligent', 'humor', 'excellent', 'performing', 'i', \"can't\", 'believe', 'how', 'people', 'could', 'think', 'it', 'deserves', 'a', '1/10!', 'i', 'hope', 'this', 'movie', 'will', 'be', 'shown', 'everywhere', 'so', 'everyone', 'can', 'enjoy', 'it', 'if', 'you', 'ever', 'have', 'the', 'opportunity,', 'watch', 'it...', \"don't\", 'miss', 'it', 'there', 'is', 'a', 'part', 'when', 'the', 'principal', 'actors', 'are', 'driving', 'and', 'singing', '\"happy', 'birthday\"', 'and', '\"el', 'payaso', 'plinplin\"', '(an', 'argentinian', 'song', 'for', 'kids', '(i', 'think...', 'it', 'could', 'also', 'be', 'south', 'american,', \"i'm\", 'not', 'sure)).', 'this', 'two', 'songs', 'that', 'have', 'the', 'same', 'melody...', 'but', 'people', \"don't\", 'usually', 'realize', 'that...', \"it's\", 'just', 'grate!', 'i', 'tried', 'to', 'write', 'this', 'in', 'both', 'spanish', 'and', 'english,', 'because', \"it's\", 'an', 'argentinian', 'movie...', 'but', 'the', 'page', \"wouldn't\", 'allow', 'me', ':(', 'hope', 'you', 'enjoy', 'it!'], tags=[10])"
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(tokenized_train_data)]\n",
    "documents[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DocModel(docs=documents, **dm_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 35s, sys: 23.5 s, total: 15min 58s\n",
      "Wall time: 7min 8s\n"
     ]
    }
   ],
   "source": [
    "%time dm.custom_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save doc2vec\n",
    "dm_doc_vec_ls = []\n",
    "for i in range(len(dm.model.docvecs)):\n",
    "    dm_doc_vec_ls.append(dm.model.docvecs[i])\n",
    "    \n",
    "dm_doc_vec = np.asarray(dm_doc_vec_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 100)"
      ]
     },
     "execution_count": 596,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm_doc_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dm_doc_vec, train_labels_enc, test_size=0.25, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_data_process(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = Feedforward(X_train.shape[1], hidden_size_in = 500, hidden_size_out = 100)\n",
    "optimizer = torch.optim.Adam(NN.parameters(), lr=0.001)\n",
    "\n",
    "train(NN,X_train,y_train,optimizer,num_epoches=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.8274666666666667\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(NN,X_test)\n",
    "print('Test accuracy: ', accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression(random_state=1, multi_class='multinomial', solver='saga')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try classification via stochastic gradient descent classifier.\n",
    "sgd = SGDClassifier(loss='hinge',\n",
    "                    verbose=1,\n",
    "                    random_state=1,\n",
    "                    learning_rate='invscaling',\n",
    "                    eta0=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.6 s, sys: 18.4 ms, total: 3.62 s\n",
      "Wall time: 3.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dochkinavika/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=1, solver='saga', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 607,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train Logistic regression\n",
    "%time logistic.fit(dm_doc_vec, np.asarray(train_labels_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8383333333333334"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = logistic.predict(dm_doc_vec)\n",
    "(preds == train_labels_enc).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 2.28, NNZs: 100, Bias: 0.551541, T: 15000, Avg. loss: 2.037334\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.84, NNZs: 100, Bias: 0.604777, T: 30000, Avg. loss: 0.733670\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.51, NNZs: 100, Bias: 0.679587, T: 45000, Avg. loss: 0.648935\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.43, NNZs: 100, Bias: 0.744350, T: 60000, Avg. loss: 0.606516\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.40, NNZs: 100, Bias: 0.793753, T: 75000, Avg. loss: 0.578942\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.23, NNZs: 100, Bias: 0.842357, T: 90000, Avg. loss: 0.555265\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.27, NNZs: 100, Bias: 0.893918, T: 105000, Avg. loss: 0.548313\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.18, NNZs: 100, Bias: 0.929273, T: 120000, Avg. loss: 0.535399\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.20, NNZs: 100, Bias: 0.940310, T: 135000, Avg. loss: 0.519121\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.17, NNZs: 100, Bias: 0.969285, T: 150000, Avg. loss: 0.514160\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.18, NNZs: 100, Bias: 0.996817, T: 165000, Avg. loss: 0.511578\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.10, NNZs: 100, Bias: 1.009003, T: 180000, Avg. loss: 0.506515\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1.09, NNZs: 100, Bias: 1.030185, T: 195000, Avg. loss: 0.502606\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1.08, NNZs: 100, Bias: 1.052342, T: 210000, Avg. loss: 0.496995\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1.15, NNZs: 100, Bias: 1.061080, T: 225000, Avg. loss: 0.490703\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 1.03, NNZs: 100, Bias: 1.084050, T: 240000, Avg. loss: 0.491501\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1.04, NNZs: 100, Bias: 1.111920, T: 255000, Avg. loss: 0.485334\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 1.07, NNZs: 100, Bias: 1.123548, T: 270000, Avg. loss: 0.480820\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 1.08, NNZs: 100, Bias: 1.153911, T: 285000, Avg. loss: 0.477021\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 1.06, NNZs: 100, Bias: 1.144834, T: 300000, Avg. loss: 0.476077\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 1.13, NNZs: 100, Bias: 1.166618, T: 315000, Avg. loss: 0.467206\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 1.00, NNZs: 100, Bias: 1.184273, T: 330000, Avg. loss: 0.471301\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 1.02, NNZs: 100, Bias: 1.194696, T: 345000, Avg. loss: 0.472871\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 1.06, NNZs: 100, Bias: 1.194733, T: 360000, Avg. loss: 0.467880\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 1.01, NNZs: 100, Bias: 1.203174, T: 375000, Avg. loss: 0.467202\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 0.96, NNZs: 100, Bias: 1.220984, T: 390000, Avg. loss: 0.465602\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 1.01, NNZs: 100, Bias: 1.235300, T: 405000, Avg. loss: 0.464354\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 0.97, NNZs: 100, Bias: 1.247825, T: 420000, Avg. loss: 0.464560\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 1.04, NNZs: 100, Bias: 1.260006, T: 435000, Avg. loss: 0.458129\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 1.00, NNZs: 100, Bias: 1.269115, T: 450000, Avg. loss: 0.460952\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 0.97, NNZs: 100, Bias: 1.273502, T: 465000, Avg. loss: 0.459800\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 0.97, NNZs: 100, Bias: 1.274964, T: 480000, Avg. loss: 0.457831\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 0.97, NNZs: 100, Bias: 1.279264, T: 495000, Avg. loss: 0.458049\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 0.94, NNZs: 100, Bias: 1.280673, T: 510000, Avg. loss: 0.455345\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 0.97, NNZs: 100, Bias: 1.280689, T: 525000, Avg. loss: 0.451544\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 0.95, NNZs: 100, Bias: 1.286218, T: 540000, Avg. loss: 0.451648\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 0.96, NNZs: 100, Bias: 1.287649, T: 555000, Avg. loss: 0.450775\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 0.94, NNZs: 100, Bias: 1.299676, T: 570000, Avg. loss: 0.451243\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 0.95, NNZs: 100, Bias: 1.299679, T: 585000, Avg. loss: 0.452803\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 0.92, NNZs: 100, Bias: 1.304891, T: 600000, Avg. loss: 0.451946\n",
      "Total training time: 0.20 seconds.\n",
      "Convergence after 40 epochs took 0.20 seconds\n",
      "CPU times: user 213 ms, sys: 12 ms, total: 225 ms\n",
      "Wall time: 212 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=1, fit_intercept=True,\n",
       "              l1_ratio=0.15, learning_rate='invscaling', loss='hinge',\n",
       "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "              power_t=0.5, random_state=1, shuffle=True, tol=0.001,\n",
       "              validation_fraction=0.1, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 609,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train SGD\n",
    "%time sgd.fit(dm_doc_vec, np.asarray(train_labels_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8303333333333334"
      ]
     },
     "execution_count": 610,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = sgd.predict(dm_doc_vec)\n",
    "(preds == train_labels_enc).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Impact of dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train document embeddings from the Section 2.3 with different number of dimensions and plot dependence of classification accuracy from the number of dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Log Regression</th>\n",
       "      <th>Feedforward NN</th>\n",
       "      <th>SGD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>word2wec on 100d</td>\n",
       "      <td>0.761</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Glove word embeddings on 100d</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.765</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Glove word embeddings on 200d</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.81</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Doc_vec on 200d</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Log Regression Feedforward NN   SGD\n",
       "word2wec on 100d                        0.761              -     -\n",
       "Glove word embeddings on 100d           0.789          0.765     -\n",
       "Glove word embeddings on 200d           0.818           0.81     -\n",
       "Doc_vec on 200d                         0.838          0.827  0.83"
      ]
     },
     "execution_count": 647,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = [[0.761, '-' ,'-' ], [0.789, 0.765, '-'],[0.818,0.81, '-' ] , [0.838,0.827, 0.83 ]]\n",
    "pd.DataFrame(data,index= [ 'word2wec on 100d','Glove word embeddings on 100d','Glove word embeddings on 200d','Doc_vec on 200d '], columns=[\"Log Regression\", \"Feedforward NN\", 'SGD'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
